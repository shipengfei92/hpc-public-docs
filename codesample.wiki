= OpenMP示例 =

= MPI示例 =

这部分演示如何使用Intel编译器编译一个名为<code>mpihello</code>的MPI程序，然后在本地测试运行，最后向LSF作业管理系统提交正式作业。 使用Environment Module可以简化环境参数设定，我们也会给出使用module的过程。

== 编译源代码 ==

<code>mpihello</code>程序的源文件为<code>mpihello.c</code>，内容如下：

<pre>#include &lt;mpi.h&gt;
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;netdb.h&gt;

#define MAX_HOSTNAME_LENGTH 256

int main(int argc, char *argv[])
{
    int pid;
    char hostname[MAX_HOSTNAME_LENGTH];

    int numprocs;
    int rank;

    int rc;

    /* Initialize MPI. Pass reference to the command line to
     * allow MPI to take any arguments it needs
     */
    rc = MPI_Init(&amp;argc, &amp;argv);

    /* It's always good to check the return values on MPI calls */
    if (rc != MPI_SUCCESS)
    {
        fprintf(stderr, &quot;MPI_Init failed\n&quot;);
        return 1;
    }

    /* Get the number of processes and the rank of this process */
    MPI_Comm_size(MPI_COMM_WORLD, &amp;numprocs);
    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);

    /* let's see who we are to the &quot;outside world&quot; - what host and what PID */
    gethostname(hostname, MAX_HOSTNAME_LENGTH);
    pid = getpid();

    /* say who we are */
    printf(&quot;Rank %d of %d has pid %5d on %s\n&quot;, rank, numprocs, pid, hostname);
    fflush(stdout);

    /* allow MPI to clean up after itself */
    MPI_Finalize();
    return 0;
}</pre>
我们使用Intel MPI并行开发环境提供的<code>mpiicc</code>对其编译, <code>mpiicc</code>会自动调用后端编译器<code>icc</code>。 使用icc前需要运行脚本使环境变量生效。

<pre>$ source /lustre/utility/intel/composer_xe_2013.3.163/bin/compilervars.sh intel64
$ source /lustre/utility/intel/mkl/bin/intel64/mklvars_intel64.sh
$ source /lustre/utility/intel/impi/4.1.1.036/bin64/mpivars.sh
$ mpicc -o mpihello mpihello.c</pre>
== 使用<code>mpirun</code>在本地测试运行 ==

测试运行需要准备<code>machinefile</code>，这个文件用来指定程序运行的主机。 我们仅在本机做测试运行，因此machinefile内容只有<code>localhost</code>。

<pre>$ echo &quot;localhost&quot; &gt; machinefile.txt</pre>
<code>mpirun</code>用于启动MPI并行程序。 下面的命令启动<code>mpihello</code>并行程序，分配4个线程。 ''注意：在<math>\pi</math>集群中，<code>mpirun</code>只能用作小于4线程的测试，大规模正式作业必须提交到LSF。''

<pre>$ mpirun -np 4 -machinefile machinefile.txt ./mpihello
Rank 0 of 4 has pid 90037 on mu05
Rank 1 of 4 has pid 90038 on mu05
Rank 2 of 4 has pid 90039 on mu05
Rank 3 of 4 has pid 90040 on mu05</pre>
== 提交到LSF作业管理系统 ==

大规模正式作业必须提交到LSF作业管理系统。<br />提交的作业由LSF作业管理系统统一调度运行，用户不需要手动书写<code>machinefile</code>。 作业控制脚本<code>mpihello.lsf</code>内容如下：

<pre>#BSUB -J HELLO_MPI
#BSUB -o job.out
#BSUB -e job.err
#BSUB -n 256
        
source /lustre/utility/intel/composer_xe_2013.3.163/bin/compilervars.sh intel64
source /lustre/utility/intel/mkl/bin/intel64/mklvars_intel64.sh
source /lustre/utility/intel/impi/4.1.1.036/bin64/mpivars.sh    

MPIRUN=`which mpirun`
EXE=&quot;./mpihello&quot;

cat /dev/null &gt; nodelist
NP=0

for host in `echo $LSB_MCPU_HOSTS |sed -e 's/ /:/g'|  sed 's/:n/\nn/g'`
do
echo $host &gt;&gt; nodelist
echo $host | cut -d &quot;:&quot; -f1 &gt;&gt; nodes
nn=`echo $host | cut -d &quot;:&quot; -f2`
NP=`echo $NP+$nn | bc`
done

$MPIRUN -np $NP -machinefile nodelist $EXE</pre>
将作业提交到名为<code>cpu</code>的队列上：

<pre>$ bsub -q cpu &lt; mpihello.lsf</pre>
作业运行结束后，在工作目录下前查看输出结果<code>job.out</code>、<code>job.err</code>、<code>mpihello.log</code>。

== 使用module简化环境设置 ==

使用Environment Module可以简化环境变量的书写，过程如下。

<ol style="list-style-type: decimal;">
<li><p>编译源程序。</p>
<pre>$ module load icc/13.1.1
$ module load mkl/11.0.3
$ module load impi/4.1.1.036
$ mpiicc -o mpihello mpihello.c </pre></li>
<li><p>在本地测试运行。</p>
<pre>$ mpirun -np 4 ./mpihello</pre></li>
<li><p>提交到LSF作业管理系统，所用的作业控制脚本如下。''注意，增加了<code>BSUB -L</code>、<code>MODULEPATH</code>、<code>module</code>若干行。''</p>
<pre>#BSUB -J HELLO_MPI
#BSUB -L /bin/bash
#BSUB -o job.out
#BSUB -e job.err
#BSUB -n 256

MODULEPATH=/lustre/utility/modulefiles:$MODULEPATH
module load icc/13.1.1
module load mkl/11.0.3
module load impi/4.1.1.036

MPIRUN=`which mpirun`
EXE=&quot;./mpihello&quot;

cat /dev/null &gt; nodelist
NP=0

for host in `echo $LSB_MCPU_HOSTS |sed -e 's/ /:/g'|  sed 's/:n/\nn/g'`
do
echo $host &gt;&gt; nodelist
echo $host | cut -d &quot;:&quot; -f1 &gt;&gt; nodes
nn=`echo $host | cut -d &quot;:&quot; -f2`
NP=`echo $NP+$nn | bc`
done

$MPIRUN -np $NP -machinefile nodelist $EXE</pre></li></ol>

提交作业：

<pre>    $ busb -q cpu &lt; mpihelllo.lsf</pre>
= CUDA示例 =

这部分演示如何编译NVIDIA CUDA程序，并提交到LSF的gpu队列中运行。 ''注意：登录节点只有CUDA软件开发环境，没有CUDA硬件加速卡，因而不能在登录节点执行CUDA应用程序，必须把作业提交到LSF的gpu队列运行。''

== 编译源代码 ==

示例的CUDA源程序名为<code>cudahello.cu</code>，内容如下：

<pre>#include &lt;stdio.h&gt;

const int N = 7;
const int blocksize = 7;

__global__
void hello(char *a, int *b)
{
 a[threadIdx.x] += b[threadIdx.x];
}

int main()
{
 char a[N] = &quot;Hello &quot;;
 int b[N] = {15, 10, 6, 0, -11, 1, 0};

 char *ad;
 int *bd;
 const int csize = N*sizeof(char);
 const int isize = N*sizeof(int);

 printf(&quot;%s&quot;, a);

 cudaMalloc( (void**)&amp;ad, csize );
 cudaMalloc( (void**)&amp;bd, isize );
 cudaMemcpy( ad, a, csize, cudaMemcpyHostToDevice );
 cudaMemcpy( bd, b, isize, cudaMemcpyHostToDevice );

 dim3 dimBlock( blocksize, 1 );
 dim3 dimGrid( 1, 1 );
 hello&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(ad, bd);
 cudaMemcpy( a, ad, csize, cudaMemcpyDeviceToHost );
 cudaFree( ad );

 printf(&quot;%s\n&quot;, a);
 return EXIT_SUCCESS;
}</pre>
我们使用NVIDIA CUDA SDK提供的<code>nvcc</code>对其编译。 使用<code>nvcc</code>前需要设置一些环境变量。

<pre>$ export PATH=/lustre/utility/cuda-5.0/bin/:$PATH
$ export C_INCLUDE_PATH=/lustre/utility/cuda-5.0/include/:$C_INCLUDE_PATH
$ export CPLUS_INCLUDE_PATH=/lustre/utility/cuda-5.0/include/:$CPLUS_INCLUDE_PATH
$ export LIBRARY_PATH=/lustre/utility/cuda-5.0/lib64/:$LIBRARY_PATH
$ export LD_LIBRARY_PATH=/lustre/utility/cuda-5.0/lib64/:$LD_LIBRARY_PATH
$ nvcc -o cudahello cudahello.cu</pre>
== 提交到LSF作业系统的gpu队列运行 ==

由于登录节点没有安装CUDA加速卡，因此不能运行CUDA程序。 CUDA程序必须提交到LSF作业管理系统的gpu队列运行。 用于提交单节点CUDA作业的LSF作业控制脚本<code>cudahello.lsf</code>内容如下：

<pre>#BSUB -J HELLO_CUDA
#BSUB -L /bin/bash
#BSUB -o job.out
#BSUB -e job.err
#BSUB -n 1
        
export PATH=/lustre/utility/cuda-5.0/bin/:$PATH
export C_INCLUDE_PATH=/lustre/utility/cuda-5.0/include/:$C_INCLUDE_PATH
export CPLUS_INCLUDE_PATH=/lustre/utility/cuda-5.0/include/:$CPLUS_INCLUDE_PATH
export LIBRARY_PATH=/lustre/utility/cuda-5.0/lib64/:$LIBRARY_PATH
export LD_LIBRARY_PATH=/lustre/utility/cuda-5.0/lib64/:$LD_LIBRARY_PATH

./cudahello &gt;&amp; cudahello.log</pre>
将CUDA作业提交到<code>gpu</code>队列上：

<pre>$ bsub -q gpu &lt; cudahello.lsf</pre>
作业运行结束后，在工作目录下查看输出结果<code>job.out</code>、<code>job.err</code>、<code>cudahello.log</code>。

== 使用module简化环境设置 ==

使用Environment Module可以简化环境变量的书写，过程如下。

<ol style="list-style-type: decimal;">
<li><p>编译源程序。</p>
<pre>$ module load cuda/5.0
$ nvcc -o cudahello cudahello.cu</pre></li>
<li><p>提交到LSF的gpu队列，使用的作业脚本为<code>cuda.lsf</code>，内容如下：</p>
<pre>#BSUB -J HELLO_CUDA
#BSUB -L /bin/bash
#BSUB -o job.out
#BSUB -e job.err
#BSUB -n 1

MODULEPATH=/lustre/utility/modulefiles:$MODULEPATH
module load cuda/5.0

./cudahello &gt;&amp; cudahello.log</pre></li></ol>

提交到gpu队列：

<pre>    $ bsub -q gpu &lt; cuda.lsf</pre>
= 参考资料 =

* “LLNL Tutorials: Message Passing Interface (MPI)” https://computing.llnl.gov/tutorials/mpi/
* “mpihello by ludwig Luis Armendariz” https://github.com/ludwig/examples
* “How to compile and run a simple CUDA Hello World” http://www.pdc.kth.se/resources/computers/zorn/how-to/how-to-compile-and-run-a-simple-cuda-hello-world
